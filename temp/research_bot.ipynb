{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Bot\n",
    "\n",
    "> - This research bot has the capabilities of any general questions.\n",
    "> - This bot is also able to ingest research documents in '.pdf' format and answer any questions based on this research document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai langchain PyPDF2 faiss-cpu tiktoken streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"fa61d3bcde7c8965f6b10c1b4995276bb452a0a75bdd96327046d2829b60bf5e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader('./Teaching_LLM.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Teaching Large Language Models to Reason\\nwith Reinforcement Learning\\nAlex Havrilla1,2,∗,Yuqing Du4,Sharath Chandra Raparthy1,Christoforos Nalmpantis1,Jane\\nDwivedi-Yu1,Maksym Zhuravinskyi3,Eric Hambro1,∗∗,Sainbayar Sukhbaatar1,Roberta Raileanu1\\n1Meta,2Georgia Institute of Technology,3StabilityAI,4UC Berkeley\\n∗Work done during Meta internship ,∗∗Work done while at Meta\\nReinforcement Learning from Human Feedback ( RLHF ) has emerged as a dominant approach for align-\\ning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance\\nof multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization\\n(PPO ), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse\\nand dense rewards provided to the LLM both heuristically and via a learned reward model. We\\nadditionally start from multiple model sizes and initializations both with and without supervised\\nfine-tuning ( SFT) data. Overall, we find all algorithms perform comparably, with Expert Iteration\\nperforming best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is\\nsimilar to that of PPO, requiring at most on the order of 106samples to converge from a pretrained\\ncheckpoint. We investigate why this is the case, concluding that during RL training models fail to\\nexplore significantly beyond solutions already produced by SFT models. Additionally, we discuss a\\ntrade off between maj@1 and pass@96 metric performance during SFT training and how conversely\\nRL training improves both simultaneously. We then conclude by discussing the implications of our\\nfindings for RLHF and the future role of RL in LLM fine-tuning.\\nDate: March 8, 2024\\nCorrespondence: Alex Havrilla at ahavrilla3@gatech.edu\\n1 Introduction\\nThe reasoning abilities of large language models ( LLMs ) are rapidly improving as measured by their performance\\non numerous math, science and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada et al.,\\n2023; Liang et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Mishra\\net al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Simultaneously,\\nReinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022; Ziegler et al., 2019; Ouyang et al.,\\n2022) and instruction fine-tuning (Wei et al., 2021; Mishra et al., 2021) have made significant progress in\\naligning LLMs with human preferences. Improvements in model instructability have further increased apparent\\nmodel capability by making complex behaviors more accessible via instruction prompting. This has led to a\\nnumber of increasingly sophisticated prompting strategies augmenting LLM reasoning capabilities such as\\nChain-of-Thought (Wei et al., 2022) or Tree-of-Thoughts (Yao et al., 2023).\\nPrevious work in reinforcement learning (RL) such as AlphaGo (Silver et al., 2017), AlphaStar (Vinyals et al.,\\n2019), and OpenAI Dota 2 (Berner et al., 2019) demonstrate that RL techniques can be used to train neural\\nnetworks capable of sophisticated planning and reasoning in game environments. Cicero (Bakhtin et al., 2022)\\nin particular succeeds in combining an RL trained planning agent with a dialogue fine-tuned LLM to achieve\\nnearly super-human performance in the board game Diplomacy. Given these previous successes and the\\ninherent interactive nature of problem solving, applying RL to LLM reasoning seems a natural next step. In\\nthis paper, we study how ideas from RL can be used to improve the reasoning capabilities of LLMs across a\\nvariety of reward schemes and model initializations.\\nWe begin by comparing the performance of different RL algorithms on reasoning tasks τdefined as a\\ndistribution of question answer tuples ( Q, A). The task τcan be extended to define a Markov Decision Process\\n(MDP ) 4-tuple ( S,A, Pa, Ra) where tokens serve as both actions and accumulated state with deterministic\\n1arXiv:2403.04642v1  [cs.LG]  7 Mar 2024dynamics. By default we use a sparse reward of +1 if the final answer is correct but also experiment with\\ndense rewards matching intermediate steps in a reference solution and rewards synthetically generated using a\\nreward model. We evaluate models with 7B and 13B parameters both starting from supervised fine-tuned\\n(SFT) checkpoints and pre-trained checkpoints. We report four metrics assessing model performance on a\\ntask specific test set: 1) maj@1 score computed by greedily sampling once per question, 2) maj@96 score\\ncomputed by sampling K = 96 times per question and uniformly voting on the final answer, 3) rerank@96\\nscore computed by sampling K = 96 times and choosing the final answer using an Outcome-Based Reward\\nModel ( ORM ), and 4) pass@96 score computed by sampling the model K = 96 times and taking the best\\nresult according to the ground truth answer.\\nWe find that overall the simplest method, Expert Iteration ( EI) (Anthony et al., 2017), performs best across\\nall metrics for most reward setups and model initializations. Surprisingly, EI is nearly as sample efficient as\\nmore sophisticated algorithms like Proximal Policy Optimization ( PPO ), both requiring only a few thousand\\nsamples to converge even when initialized from a pretrained checkpoint. We also observe the gap between\\npretrained model performance and SFT model performance significantly shrinks ( <10% gap on GSM8K)\\nafter RL fine-tuning, with larger models having a smaller gap. Additionally, previous work identified a tradeoff\\nbetween test time maj@1 performance and pass@96 performance during supervised fine-tuning (Cobbe et al.,\\n2021), with continued training increasing maj@1 score at the expense of pass@96 score. We identify the\\nlimited diversity of the dataset as a core reason for this. We show that RL fine-tuning can improve both\\nmetrics simultaneously due to the fact that RL generates its own data during training, resulting in a more\\ndiverse set of examples to learn from.\\nWe then discuss why EI and return conditioned RL are competitive with PPO, suggesting two principal\\nfactors. Firstly, the reasoning tasks we consider have entirely deterministic dynamics: a setting in which\\ndirect behavior cloning and return conditioned RL is known to do well (Brandfonbrener et al., 2022). In\\ncontrast, PPO often succeeds in environments with a high degree of stochasticity (Bhargava et al., 2023).\\nSecond, we identify a lack of sophisticated exploration carried out by models during RL fine-tuning. This\\nlimitation significantly impacts any performance or sample complexity advantages PPO may have when\\nfine-tuning the pretrained model. We come to this conclusion from a number of observations, noting in\\nparticular quickly saturating pass@96 scores early in RL training. We conclude with a discussion of the\\nimpacts of our observations on RLHF and the future of LLM fine-tuning via RL.\\nIn summary we make the following contributions:\\n•A comprehensive study of PPO fine-tuning of LLMs on reasoning tasks using different types of rewards,\\nmodel sizes and initializations.\\n•A comparison to expert iteration and return-conditioned RL from which we find expert iteration reliably\\nattains the best performance and competitive sample complexity across the board.\\n•A discussion of the implications of our findings for RLHF and the future of RL fine-tuning for LLMs,\\nidentifying exploration as a major limiting factor.\\n2 Related Work\\nLLM Reasoning: State-of-the-art large language models (OpenAI, 2023; Touvron et al., 2023; Bai et al., 2022;\\nChowdhery et al., 2022) demonstrate increasingly impressive abilties on hard reasoning tasks as studied by\\na wide range of math, science, and code benchmarks (Cobbe et al., 2021; Hendrycks et al., 2021b; Sawada\\net al., 2023; Liang et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019;\\nMishra et al., 2022; Hendrycks et al., 2021a; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). Chain\\nof thought (CoT) (Wei et al., 2022) and related techniques (Chen et al., 2022; Yao et al., 2023; Besta et al.,\\n2023) have emerged as dominant methods siginficantly boosting LLM performance on these types of tasks.\\nCoT methods allow LLMs to defer giving their final answer by first generating a ”chain of thought” involving\\nintermediate computations needed to correctly solve the problem.\\nAnother line of work combines base LLM reasoning capabilities with planning and search algorithms to further\\nboost performance on a wide range of tasks (Yao et al., 2023; Besta et al., 2023; Ye et al., 2022; Yao et al.,\\n22022; Dohan et al., 2022). Tree of thought (Yao et al., 2023) for example combines LLMs with a breadth first\\nsearch algorithm, relying on the LLM to both propose actions and evaluate state. Other works combine LLMs\\nwith tools (Schick et al., 2023; Qin et al., 2023; Zhou et al., 2023a) further boosting reasoning capability.\\nCombining GPT-4 with a python code interpreter for generation and self-verification achieves an impressive\\n84% on the hard MATH benchmark (Hendrycks et al., 2021a; Zhou et al., 2023a).\\nOther works focus on LLMs for mathematical reasoning in natural language (Cobbe et al., 2021; Lewkowycz\\net al., 2022; Azerbayev et al., 2023; Lightman et al., 2023; Patel et al., 2021; Zhu et al., 2023; Rafailov\\net al., 2023). Particularly relevant to our study is Cobbe et al. (2021) which fine-tunes GPT-3 on supervised\\nmath word problem ( MWP ) reasoning traces. In addition they train solution verifiers called Outcome Based\\nReward Models ( ORMs ) which predict the probability of correctly solving a question Qgiving a prefix of\\nintermediate steps Pi= (S1, ..., S i) i.e. p(iscorrect (A)|Q, P i) where Ais a solution with prefix Pi. Process\\nbased reward models ( PRMs ) (Uesato et al., 2022; Lightman et al., 2023) can also be trained to instead look\\nat the step-level accuracy of solutions. More recent work (Luo et al., 2023) utlizies a PRM distilled from\\nGPT-4 feedback as a reward signal during PPO.\\nRL for LLM fine-tuning: Reinforcement Learning from Human Feedback (RLHF) is perhaps the most\\nwell-known application of RL techniques for fine-tuning LLMs. RLHF (Christiano et al., 2017; Ziegler et al.,\\n2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022; Peng et al., 2021;\\nRamamurthy et al., 2022) most often works by training a reward model to capture human preferences over\\na task τ. The reward model is then used to score LLM responses to prompts from the task after which\\npolicy improvement is performed. PPO is most often used (Ouyang et al., 2022; Bai et al., 2022) but several\\nrecent works including ReST (Gulcehre et al., 2023), Reward-Ranked Fine-tuning (Dong et al., 2023), and\\nAlpacaFarm (Dubois et al., 2023) all demonstrate simply fine-tuning on high return responses with the\\nstandard cross-entropy loss can attain comparable performance. We broadly refer to this class of algorithms\\nas Expert Iteration.\\nA large body of work studying RL for LLM fine-tuning also exists outside of the RLHF sphere. Work on text\\ngames (Yao et al., 2020; Ammanabrolu and Riedl, 2019) and other interactive textual environments (Zhou\\net al., 2023b; Carta et al., 2023) seek to ground LLMs via interaction and RL. RL has also been applied to\\nimproving model performance on controllable generation and question answering tasks (Lu et al., 2022; Liu\\net al., 2022). Various forms of expert iteration have also been applied to improve LLM reasoning capabilities\\n(Huang et al., 2022; Yuan et al., 2023; Zelikman et al., 2022; Uesato et al., 2022). For example “Scaling\\nRelationship on Learning Mathematical Reasoning with Large Language Models” (Yuan et al., 2023) applies\\na single round of expert iteration across multiple model sizes on GSM8K. They observe sizeable gains in all\\nmetrics for smaller models, with gains diminishing for larger models. A related body of work studies RL\\nfor code generation (Le et al., 2022; Shen et al., 2023; Rozi` ere et al., 2023). Shen et al. (2023) in particular\\nreports a huge increase in StarCoder’s (Li et al., 2023) maj@1 performance after a single round of expert\\niteration, jumping from ∼30% to ∼60%.\\nDespite all the above work, it remains unclear exactly what factors account for the biggest impact during RL\\nfine-tuning due to wide variance in tasks, pretraining data, supervised fine-tuning data, RL algorithm used,\\nand the reward source. Our work conducts a thorough analysis of all these factors to understand exactly how\\ndifferent algorithms compare when applied to improving LLM reasoning capability. As a result we are able to\\nidentify key bottlenecks to further LLM improvement via RL and provide a discussion on promising future\\ndirections.\\n3 Methods\\nReasoning as an RL problem\\nWe study the performance and sample complexity requirements for various RL algorithms when fine-tuning\\nLLMs on reasoning tasks. We consider Expert Iteration (EI) (Anthony et al., 2017), Proximal Policy\\nOptimization (PPO) (Schulman et al., 2017), and Return-Conditioned RL (RCRL) (Brandfonbrener et al.,\\n2022) as representative algorithms from the RL literature. In general, the goal of all RL algorithms is to\\nmaximize the expected future return EA∼π(Q),(Q,·)∈τR(A) of a student policy πon task τ. We call the highest\\nreturn policy the optimal policy π∗. Each of our chosen algorithms goes about finding π∗in a different way.\\n3PPO is an example of an online RL algorithm. Online algorithms engage in both an exploration phase and a\\npolicy improvement phase which updates πθusing data generated during the exploration phase. PPO is also\\nanon-policy algorithm which samples model rollouts during exploration from the student policy πθbeing\\ntrained. During policy improvement, the student πθupdates its parameters via gradient descent by directly\\nmaximizing for reward with the objective\\nJ(θ) =Et\\x14\\nmin(π(at|st)\\nπold(at|st)ˆAt, clip(1−ϵ,1 +ϵ,π(at|st)\\nπold(at|st))ˆAt)\\x15\\nwhere ˆAtestimates the advantage i.e. difference between Q(s, a) (the expected return after taking action aat\\nstate s) and value V(s) (the expected return at state s).\\nIn practice, for PPO we sample 1024 rollouts at a time with a temperature of 0.7 and N= 4 rollouts per\\nquestion. Training is then run on these samples for K= 4 PPO epochs with a batch size of 256. Additionally,\\nwe train using LoRA (Hu et al., 2021) with r= 128. Training is run for 4000 gradient steps. The best\\ncheckpoint is then selected via performance on a validation set.\\nExpert iteration is also online but more off-policy than PPO. An initial expert policy approximation ˆπ∗\\n0\\nis sampled on the entire train set Ktimes per question before any policy improvement. The ˆπ∗\\n0is often\\nconstructed using repeated sampling from an initial policy π0. For example, AlphaZero (Silver et al., 2017)\\nand subsequent work (Schick et al., 2023) combine π0with Monte Carlo Tree Search. Sampling ˆπ∗\\n0constructs\\nan initial set of rollouts D1which are then distilled back into a policy π1via a standard cross-entropy\\nloss:P\\nτ∈DPH\\nt=1−log(πθ(at|st)). This process can be repeated to construct policy πifine-tuned on dataset\\nDi=Ri∪Di−1where Ricorresponds to exploration done by πi−1.\\nIn our setting we construct an approximation to the optimal policy ˆπ∗by rejection sampling our student\\npolicy πθ. After generating Ksamples S1, ..., S Kon a question Qwe construct D1by filtering all ( Q, S i)\\npairs with return below a threshold T. De-duplication is then performed on the remaining samples.\\nIn practice, during the expert iteration exploration phase we sample each question in the train set K= 96\\ntimes with temperature T= 1.0. To construct the training set we filter out incorrect solutions and duplicates.\\nImportantly, fine-tuning is then done from the pretrained base model with the same hyperparameters as SFT.\\nThis is repeated until performance on a validation set saturates.\\nReturn Conditioned RL Return conditioned RL algorithms seek to train policies conditioned on both the\\ncurrent state sand desired return Rwhen sampling an action. This is motivated by a desire to learn return\\nconditionable policies which can change depending on the desired return. Best performance can then be\\nsampled by conditioning on the highest possible return.\\nWe consider an offline version of this class of algorithms similar to a decision transformer (Chen et al.,\\n2021). A training dataset Dis constructed by generating state, action, return τ= ((st, at, gt))H\\nt=1trajectories.\\nTraining is done by predicting the action given state and return:P\\nτ∈DPH\\nt=1−log(πθ(at|st, gt)). In practice\\nwe construct Dby sampling solutions S= (S1, ..., S L), where each Siis an intermediate step, from our best\\nEI trained policy πEIgiven a question Q. We generate return labels for each step Siby sampling πEIK many\\ntimes from Pi= (S1, ..., S i). This results in binary labels l1, .., lKevaluating the correctness of the generated\\nfinal answers. Siis then labeled as “[GOOD]” if the average return1\\nKPK\\nk=1lk≥Tand otherwise is labeled\\nas “[BAD]”. Typically we set T= 0.5. We then filter the dataset to ensure a balanced number of correct and\\nincorrect solutions. See Section F in the appendix for more details about the step-label generating process.\\nOutcome Based Reward Modeling Multiple works (Cobbe et al., 2021; Uesato et al., 2022) train Outcome\\nBased Reward models ORMs asverifiers of candidate solutions to word problems. The ORM can then be\\nused to rerank multiple candidate solutions generated by a student model, significantly boosting performance.\\nTraining data for the ORM is generated using a student policy πby sampling Ksolutions per question Qin\\nthe task dataset. The ORM is trained as a classifier by predicting the probability of reaching the correct final\\nanswer p(iscorrect(A) |Q, P i) from an intermediate sequence of steps Pi= (S1, ..., S i),Pi⊆A= (S1, ..., S L).\\n4maj@1 maj@96 rerank@96†pass@96\\n7B 13B 7B 13B 7B 13B 7B 13B\\nSFT 0.41 0.48 0.47 0.53 0.54 0.68 0.72 0.84\\nEIn 0.48 0.53 0.55 0.59 0.64 0.71 0.8 0.88\\nORM EI n 0.48 0.53 0.54 0.58 0.65 0.71 0.81 0.87\\nORM RCRL 0.45 0.51 0.5 0.56 0.54 0.69 0.73 0.83\\nSparse PPO 0.44 0.51 0.49 0.55 0.58 0.67 0.77 0.85\\nDense PPO 0.43 0.50 0.47 0.54 0.53 0.65 0.71 0.81\\nSparse ORM PPO 0.46 0.51 0.51 0.55 0.59 0.67 0.79 0.83\\nDense ORM PPO 0.46 0.51 0.52 0.55 0.59 0.67 0.76 0.83\\nLlema∗0.40 0.62 0.54 0.69 N/A N/A\\nRFT 0.47 0.54 0.58 0.65 N/A N/A\\nWizardMath 0.55 0.64 N/A N/A N/A\\nGPT-3∗∗0.2 0.31 N/A 0.39 0.55 0.71 NA\\nGPT-4∗∗∗0.91 N/A N/A N/A\\nTable 1 Results when initializing from SFT. EI ndenotes n rounds of expert iteration until convergence with n= 2 for\\n7B and n= 2 for 13B.†Note all reranking is done using an ORM trained with samples from EI n. Results from other\\nworks are included on the bottom for reference. N/A stands for not available.∗Llema results reported for 7B/34B\\nsizes without fine-tuning.∗∗GPT-3 results reported for 7B/175B sizes.∗∗∗GPT-4 size unknown.\\n4 Experiments\\nWe conduct our evaluations on GSM8K and SVAMP (Patel et al., 2021): two math word problem benchmarks.\\nIn addition on GSM8K we consider two data regimes: first with SFT data and then without SFT data. We\\nevaluate all models using greedy sampling (maj@1) accuracy as well majority vote at 96 samples (maj@96),\\nORM based reranking at 96 samples (rerank@96), and best of 96 sample (pass@96) accuracy. Unless otherwise\\nspecified, test-time sampling is done greedily for maj@1 and with a temperature of 0.7 otherwise. We sample\\nthe RCRL models one step/line at a time, conditioning on the “[GOOD]” token. We note while the notion of\\na “step” is not clearly defined in general, in our case we can simply regard each step as ending with a sentence\\nor newline. All experiments are done using instruction-tuned Llama-2 7B and Llama-2 13B models.\\n4.1 Results with SFT Initialization\\nWhen given access to SFT data, we first supervise fine-tune Llama-2 models for 4 epochs with a global batch\\nsize of 128 and an initial lr of 2e-5 decayed to 2e-7 with a cosine warmup schedule. We call the resulting\\nmodels SFT. When fine-tuning with PPO we initialize using this checkpoint. In contrast, for both EI and\\nRCRL we generate data with the SFT checkpoint but reset training to start from the pretrained base model.\\nSimilarly to Zelikman et al. (2022), we find this model resetting is crucial for achieving best performance.\\nResults for both 7B and 13B models are reported in Table 1.\\nExpert iteration achieves the best performance with competitive sample complexity\\nSurprisingly, we find EI achieves the best performance with a maj@1 accuracy of 0.485 and 0.53 on 7B and 13B\\nmodels respectively. For both model sizes the best greedy accuracy is achieved after n= 2 expert iterations\\n(see Fig. 2), after which performance plateaus. In total, EI gives a sizable improvement of around 7% over\\nthe SFT baseline. Similar gains can be seen in maj@96, rerank@96, and pass@96 scores with.\\nPPO models underperform EI, with ORM guided PPO giving the biggest improvement of around 5% over the\\nSFT baseline. Again, maj@96, rerank@96, and pass@96 accuracies show similar improvements. Interestingly,\\ndespite further training on top of the SFT initialization, PPO models retain competitive rerank@96 and\\npass@96 scores when compared to regression we see after further supervised fine-tuning. We believe this is\\ndue to the relatively more diverse nature of the exploration dataset used to update the model.\\nFinally, RCRL models under-perform EI models despite training on EI generated data with an even balance\\n5Figure 1 Sample complexities of SFT initialized models\\non GSM8K. EI achieves better performance than PPO\\nwith the same order of magnitude of samples.\\nFigure 2 Accuracy of EI models on GSM8K test vs.\\nnumber of iterations. Performance seems plateaus for SFT\\ninitialized models after two iterations. The pretrained\\ncheckpoints converge after four iterations.\\nbetween ‘[GOOD]’ and ‘[BAD]’ step labels. This matches similar results from Du et al. (2023) which use only\\nsparse labels for the entire rollout. Further, when sampling the RCRL model unconditionally the model often\\ngenerates the perfectly valid steps following a ‘[BAD]’ label resulting in a correct final answer. These results\\nsuggest RCRL models are not correctly learning what constitutes a ‘[GOOD]’ versus ‘[BAD]’. This suggests\\nRCRL models are unable to usefully incorporate information from partially correct solutions at train time.\\nAn ablation (See sec. A of the appendix) on the ratio of positive to negative labels finds a balanced ratio\\nyields the worst performance, with increasing the amount of positive data leading to better results.\\nIn Figure 1 we plot the number of model rollouts against model performance in log-scale. PPO models achieve\\ntheir best accuracies after around 60,000 rollouts while EI models train with an order of magnitude more.\\nHowever, the resulting train time in both cases is about a day. This is largely due to memory requirements\\nfrom PPO, resulting in lower rollout throughput and smaller mini-batch sizes at train time. Additionally, in\\nthe SFT case we did not experiment with reducing the number of samples from K= 96 per question for EI.\\nHowever, we expect this number can be significantly reduced without impacting performance. For a more\\nthorough investigation of sample complexity requirements, see Figure 5.\\nExtra guidance from ORMs or dense rewards provides little benefit Overall, the ORM slightly improves\\nPPO performance and negligibly impacts EI performance. For both algorithms it provides an improvement in\\nterms of sample complexity. However, this does not change final performance. See Figures 3 and 4 which plot\\nthe performance against number of model rollouts for differnt reward regimes.\\nGiving dense rewards at best provides no extra benefit to performance when given either heuristically or\\nvia the ORM. Giving a heuristic dense reward even slightly harms model performance relative to the sparse\\nsetting. Recall we give intermediate reward by comparing intermediate model generated steps to the reference\\nsolution. This likely encourages more overfit to exact solutions in the train set, limiting solution diversity.\\nRL improves maj@1 accuracy without impacting pass@96 performance Looking at the pass@96 accuracies\\nmore closely, we see most similarly sized models are within 3% of the best result. This demonstrates with\\nenough sampling, most models are able to solve a very similar range of problems. Further, while the pass@96\\naccuracy of our best EI model initially seems much higher than the SFT checkpoint, this is only because the\\nSFT checkpoint has undergone much more training on a less diverse dataset. Simply supervised fine-tuning\\nfor half as many steps results in a checkpoint with maj@1 = 0.36 but pass@96 = 0.76. This further suggests\\nRL training mostly impacts maj@1 accuracy without significantly improving on a pass@n accuracy which can\\nbe achieved with a light amount of supervised fine-tuning.\\nThe proximity of pass@96 accuracies among most models is in sharp contrast to the rerank@96 performance.\\nHere we find EImodels enjoy around a 5% lead over other models. At first glance this seems contradictory\\n6Figure 3 maj@1 scores of EI and ORM aided EI models\\nover the course of training. The ORM improves sample\\nefficiency but not performance.\\nFigure 4 maj@1 scores of PPO and ORM guided PPO\\nmodels over the course of training. As with EI models, the\\nORM improves sample efficiency but not performance.\\nmaj@1 maj@n rerank@n†pass@n\\n7B 13B 7B 13B 7B 13B 7B 13B\\nPrompted 0.05 0.03 0.14 0.18 0.17 0.24 0.22 0.27\\nEIn 0.31 0.4 0.35 0.47 0.39 0.63 0.45 0.83\\nORM EI 0.28 0.37 0.33 0.43 0.37 0.59 0.42 0.76\\nSparse PPO 0.32 0.41 0.37 0.48 0.41 0.65 0.5 0.83\\nSparse ORM PPO 0.29 0.38 0.34 0.44 0.4 0.62 0.49 0.81\\nDense ORM PPO 0.29 0.39 0.35 0.45 0.41 0.64 0.5 0.82\\nTable 2 Results for 7B/13B models when notusing SFT initialization on GSM8K. Sparse PPO performs slightly\\nbetter than EIin this setting.∗Note all reranking is done using an ORM trained with samples from EI nmodel.\\nwith relatively similar pass@96 performance. However, we believe a non-trivial percentage of this gap is due\\ntooverfit of the ORM to the EI model which was used to generate its training data .\\n4.2 Results with no SFT Initialization\\nWe now consider the case when no SFT data is available for training. For questions in both SVAMP and\\nGSM8K we give pretrained models access to a two-shot prompt with samples drawn from the GSM8K\\nvalidation set. For EI models, we remove these prompts after the first round of exploration, instead relying on\\nthe generated SFT data. As in the case with SFT data, we run both algorithms until performance saturates.\\nFor PPO this happens after 250 steps on SVAMP and 1000 steps on GSM8K. For EI, this happens after n= 5\\nrounds of exploration and distillation. Results on both datasets are reported in Tables 2 and 3.\\nEI achieves the best performance overall Even without SFT data, EI achieves the best performance on\\nSVAMP, improving 7B/13B pretrained greedy model accuracies over 50% from 0.06/0.05 to 0.58/0.69%,\\nrespectively. PPO performs slightly better than EI on GSM8K, improving from 0.05/0.03 to 0.31/0.4. Both\\nalgorithms achieve comparable pass@96 scores across modes sizes, further supporting our observations from\\nthe SFT regime that EI mostly improves maj@1 scores relative to PPO. The prompted 13B model on GSM8K\\neven attains 0.83 pass@96 accuracy which is close to the 0.84 pass@96 score achieved by the SFT model,\\ndespite having no access to SFT data itself.\\nEI has the same sample complexity as PPO As before we plot the reward versus number of model rollouts\\nfor PPO and EI in Figures 5 and 6. On GSM8K PPO models attain their best maj@1 accuracies after only\\n30,000 rollouts and on SVAMP even less. Surprisingly, EI models have the same sample complexity as PPO on\\n7maj@1 maj@n rerank@n†pass@n\\n7B 13B 7B 13B 7B 13B 7B 13B\\nPrompted 0.06 0.05 0.2 0.25 0.24 0.29 0.3 0.36\\nEIn 0.58 0.69 0.6 0.75 0.62 0.78 0.70 0.93\\nSparse PPO 0.44 0.51 0.55 0.66 0.58 0.73 0.72 0.89\\nSparse ORM PPO 0.43 0.51 0.52 0.64 0.54 0.71 0.65 0.85\\nDense ORM PPO 0.44 0.52 0.51 0.63 0.55 0.73 0.67 0.85\\nTable 3 Results for 7B/13B models when notusing SFT initialization on SVAMP. EI ndenotes the best EI model\\nafter niterations. EI outperforms PPO.\\nFigure 5 Sample complexities on GSM8K from pretrained\\ninitialization.\\nFigure 6 Sample complexities on SVAMP. Surprisingly,\\nEI appears nearly as sample efficient as PPO.\\nSVAMP, requiring more samples to converge but also converging to a much higher accuracy. EI still appears\\nto have higher sample complexity on GSM8K, however as noted before this may be due to oversampling each\\nprompt during the exploration phase. To test this, we reduce the number of samples per prompt each round\\nof EI from K= 96 to K= 4. The resulting EI models require more iterations to converge but require far\\nless total samples, also converging in accuracy only a few percentage points lower than K= 96 samples per\\nprompt. With K= 4 rollouts per prompt EI has the same sample complexity as PPO on GSM8K.\\nThis is a particularly surprising finding when compared to the performance of EI and PPO on more classical\\nRL problems training a neural network from scratch. Often PPO enjoys far better sample complexity in\\nthese settings. One major difference here is the initialization of our student from a pretrained model which\\nimparts a very strong bias on the kind of behaviors and exploration encountered during RL training. Both\\nthe extremely small sample complexity and the comparability of EI and PPO in this setting provide more\\nevidence that models are not truly engaging in complex exploration, but instead primarily drawing on what\\nthey already know from the pre-training phase.\\n4.3 Implementation Details\\nIt is well known RL training can be quite sensitive to architectural and hyperparameter choices. This is\\neven more so the case for LLM fine-tuning. In this section we ablate and discuss the factors we found most\\nimportant in our tasks.\\nPPO model architecture and training parameters To save memory we use a joint architecture for the PPO\\npolicy and value heads. We found it important to use a relatively large value branch (L=4 transformer\\nlayers) and detach the gradients coming from the value branch to the policy trunk. Without detachment we\\nfound value gradients interfere with policy gradients, as similarly observed in Stiennon et al. (2020), causing\\n8instability with a big update to either branch. See Figure 7 which compares maj@1 score of a student with a\\nlarge value branch and detached value gradients versus the default.\\nFigure 7 maj@1 performance of PPO fine-tuned models\\nagainst architectural changes. Note, we initialize training\\nfrom a 7B SFT model with maj@1 = 0.29.\\nFigure 8 Best K of N sampling parameters versus maj@1\\nscore during training. K=4, N=4 yields a fast runtime\\nand best performance.\\nLow rank adaptation (LoRA) (Hu et al., 2021) with rank r= 128 helped significantly to further stabilize a full\\nlayer fine-tuning while still maintaining performance (Sun et al., 2023). A large enough batch size (BS = 256)\\nand a small lr = 1e-6 also helped with stabilization. We additionally experimented with a partial fine-tune of\\nonly the top M layers. This saved memory but at the cost of a few percentage points of performance.\\nWe also found a non-trivial KL penalty of 0 .05 to be critical for preventing model collapse after more than a\\nhundred gradient updates. This is in contrast to Bai et al. (2022) who do not see a significant need for the\\nKL constraint. We attribute its importance here to the somewhat unnatural distribution of text found in the\\nthe reasoning tasks which consist of broken natural language and computations enclosed in <<x+y=z>> tags.\\nFor tasks with distributions closer to pure natural language dialogue, such as those considered in Bai et al.\\n(2022), the KL constraint seems less necessary.\\nSampling parameters affect exploration We found the best temperature to use for good exploration during\\nPPO training heavily depends on the initialization. When starting from an SFT checkpoint we choose T\\n= 0.7. However, sampling on a high temperature when starting from the pretrained prompted model often\\nresults in collapse. In these cases we choose a low temperature (T = 0.2). Potentially better results for PPO\\ncould likely be achieved by annealing the exploration temperature over the course of training. We similarly\\nexperimented with the sampling temperature used during exploration in EI, ultimately deciding on T= 1.0\\nto maximize solution diversity without sampling too many degenerate solutions.\\nWe also experimented with best K of N (KoN) sampling during PPO training to promote more solution\\ndiversity. In this setup the K highest reward samples of N rollouts from a single prompt are kept for training\\nand the rest are discarded. Choosing parameters K ≪N prioritize high reward samples and discard low\\nreward ones, resulting in a training distribution more similar to the curated EI dataset.\\nHowever, one important consideration is the impact of the K/N ratio on training time and sample complexity,\\nwith smaller ratios taking proportionally longer. For example, K=1,N=8 takes 8 times as long as the default\\nK=1,N=1. Further, we ultimately found little benefit to small K/N ratios with most configurations yielding\\ndecreased performance over K=1,N=1. In practice we found setting K=4, N=4 worked best. See Figure 8\\nwhich compares the performance of various choices of K and N.\\nModel size and initialization affect exploration We found both the quality of the student initialization and\\nthe size of the student significantly affected the type of exploration engaged in during training. In particular\\nlarger models engaged in more diverse exploration while models with worse generalization engaged in less\\ndiverse exploration (See Appendix Section B). This in turn directly impacts model performance when trained\\n9maj@1 maj@96 Rerank@96 pass@96\\nSFT20.36 0.45 0.53 0.76\\nSFT40.41 0.47 0.54 0.72\\nPPO20.43 0.48 0.59 0.8\\nPPO40.44 0.49 0.58 0.77\\nTable 4 Results for full supervised fine-tune (SFT4), half supervised fine-tune (SFT2) and their PPO fine-tunes.\\nFine-tuning for only two epochs gets pass@96 = 0.76. This decreases to 0.72 with two additional epochs of fine-tuning.\\non exploration data, with models engaging in more diverse exploration improving more from RL training.\\nTo further examine the observations about overfitting, we supervise fine-tune a Llama-2-7B model for half as\\nmany steps than the SFT model reported in Table 1. We call the model trained for four epochs SFT4and the\\nmodel trained for two epochs SFT2. Despite half the training, SFT2has similar Rerank@96 and superior\\npass@96 scores to SFT4with the main difference being the maj@1 accuracies. When sampled K = 96 times on\\neach train prompt, SFT2produces on average 3.7 unique correct solutions compared to SFT4which produces\\n2.9 unique correct solutions. We also find SFT2benefits significantly more from RL fine-tuning than SFT4,\\njumping from maj@1=0.36 to maj@1=0.43. It’s important to note some of this improvement also happens\\nwith continued SFT training, however at the cost to model output diversity and pass@96 performance.\\nWe believe RL fine-tuning is less prone to overfitting when compared to static SFT fine-tuning precisely\\nbecause of the exploration process which generates its own training data. This results in in more diverse\\nsolution paths than the SFT training set, ameliorating overfit. This is also in line with recent work that found\\nRLHF to result in better (out-of-distribution) generalization than SFT on summarization and instruction\\nfollowing tasks (Kirk et al., 2023). This benefit can be found both PPO and EI which have almost 10%\\npass@96 improvement over continued SFT (yet a much smaller pass@96 improvement over a light SFT). To\\nsupport this hypothesis we plot the solution accuracies and diversities of EI models over each iteration in\\nFigures 10 and 12, respectively. Figure 12 also shows larger models generate more diverse solutions.\\n5 Discussion and Conclusions\\nOur study resulted in the following findings:\\n1.All the tested RL algorithms perform similarly on reasoning tasks, with Expert Iteration performing\\nbest in most cases.\\n2.Both EI and PPO converge relatively quickly even without supervised fine-tuning, requiring only ∼60,000\\nmodel rollouts.\\n3. Neither algorithm benefits significantly from ORM guidance or a denser reward.\\n4.EI and PPO fine-tuning simultaneously improves maj@1 score and pass@n score in contrast with SFT.\\nThe improvement of both maj@1 and pass@n performance noted above is due to the ability of online RL\\nalgorithms to dynamically grow diverse sets of training examples via synthetic data generation. This allows\\nfor longer training/more gradient updates on the same model without adversely impacting output diversity\\nand pass@n scores. In contrast, SFT training occurs on a static dataset. This limits how much training can\\noccur before maj@1 overfit occurs and output diversity suffers. However, RL training does not significantly\\nimprove pass@n score beyond what can be achieved with light supervised fine-tuning. This suggests even\\nwith RL training our best models are not discovering solutions beyond what can be discovered with (light)\\nsupervised fine-tuning given the same rollout budget.\\nThis observation, taken together with the fast convergence of both online algorithms and the low-impact of\\nORM guidance and dense rewards, suggests models are not engaging in a significant amount of exploration\\nbeyond pretraining/SFT data. Regardless of the type of algorithm used or the quality of the reward, all\\nstudent models engage in similar exploration, resulting in similar performance.\\n10Crucial in our setting is the usage of a pretrained model imparting a strong exploration prior. Without such a\\nprior, exploration in a high-dimensional textual action space would be impossible. However, this prior also\\nappears to constrain the exploration engaged in at the beginning of training, with additional SFT training\\nonly making things worse. We view the discovery of new techniques encouraging complex, rich exploration of\\nreasoning problems as fundamental to progress in LLM reasoning capability. More sophisticted prompting\\nstrategies such as Tree of Thought (Yao et al., 2023) and combining LLM generative abilities with evolutionary\\nalgorithms (Lehman et al., 2022) have already begun to make progress in this direction.\\nIn addition to the limited exploration noted above, we also note reasoning environments are entirely deter-\\nministic. This is a setting in which EI and RCRL algorithms are already known to work well theoretically\\n(Brandfonbrener et al., 2022). PPO enjoys more advantage in environemnts with a high degree of stochasticity.\\nWe also note prior work in RLHF finds PPO outperforms EI type approaches in human preference satisfaction\\nand instruction following (Gulcehre et al., 2023; Dubois et al., 2023; Kirk et al., 2023). Importantly, in our\\nsetting we always have a reliable ground truth reward to optimize. However, in RLHF, models must optimize\\nagainst an unreliable reward model, often resulting in over-optimization (Gao et al., 2022). The relatively\\nsuperior performance of PPO over EI on RLHF tasks versus reasoning tasks suggests PPO better mitigates\\nsuch over-optimization. This is not too surprising since PPO training penalizes student models diverging\\nfrom the initial policy via both its clipped objective and additional KL-constraint. In contrast, EI training\\nhas no such protection built in.\\nReferences\\nPrithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep reinforcement learning.\\nIn Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers) , pages 3557–3565, Minneapolis, Minnesota, June 2019. Association for Computational\\nLinguistics. doi: 10.18653/v1/N19-1358. URL https://aclanthology.org/N19-1358 .\\nThomas W. Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In\\nNeural Information Processing Systems , 2017. URL https://api.semanticscholar.org/CorpusID:19449905 .\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models.\\nArXiv , abs/2108.07732, 2021. URL https://api.semanticscholar.org/CorpusID:237142385 .\\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng,\\nStella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. ArXiv , abs/2310.10631,\\n2023. URL https://api.semanticscholar.org/CorpusID:264172303 .\\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna\\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez,\\nDawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, J Landau,\\nKamal Ndousse, Kamile Lukovsi¯ ut˙ e, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem’i\\nMercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, T. J. Henighan, Tristan Hume,\\nSam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B.\\nBrown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. ArXiv , abs/2212.08073, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:254823489 .\\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan\\nGray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis,\\nAlexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak,\\nAlexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of ¡i¿diplomacy¡/i¿ by\\ncombining language models with strategic reasoning. Science , 378(6624):1067–1074, 2022. doi: 10.1126/science.\\nade9097. URL https://www.science.org/doi/abs/10.1126/science.ade9097 .\\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David\\nFarhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J´ ozefowicz, Scott Gray, Catherine Olsson, Jakub W.\\nPachocki, Michael Petrov, Henrique Pond´ e de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale\\n11deep reinforcement learning. ArXiv , abs/1912.06680, 2019. URL https://api.semanticscholar.org/CorpusID:\\n209376771 .\\nMaciej Besta, Nils Blach, Alevs Kubivcek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann,\\nMichal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate\\nproblems with large language models. ArXiv , abs/2308.09687, 2023. URL https://api.semanticscholar.org/\\nCorpusID:261030303 .\\nPrajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, and Amy Zhang. Sequence modeling is a robust\\ncontender for offline reinforcement learning. ArXiv , abs/2305.14550, 2023. URL https://api.semanticscholar.\\norg/CorpusID:258866105 .\\nDavid Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-\\nconditioned supervised learning work for offline reinforcement learning? ArXiv , abs/2206.01079, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:249282285 .\\nThomas Carta, Cl´ ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding\\nlarge language models in interactive environments with online reinforcement learning. ArXiv , abs/2302.02662, 2023.\\nURL https://api.semanticscholar.org/CorpusID:256615643 .\\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor\\nMordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing\\nSystems , 2021. URL https://api.semanticscholar.org/CorpusID:235294299 .\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. ArXiv , abs/2211.12588, 2022.\\nFran¸ cois Chollet. On the measure of intelligence. ArXiv , abs/1911.01547, 2019. URL https://api.semanticscholar.\\norg/CorpusID:207870692 .\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,\\nJoshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily\\nReif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc´ ıa,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai,\\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D´ ıaz, Orhan Firat, Michele Catasta,\\nJason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\\nmodeling with pathways. J. Mach. Learn. Res. , 24:240:1–240:113, 2022. URL https://api.semanticscholar.org/\\nCorpusID:247951931 .\\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning\\nfrom human preferences. Advances in neural information processing systems , 30, 2017.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\\nTworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math\\nword problems. ArXiv , abs/2110.14168, 2021. URL https://api.semanticscholar.org/CorpusID:239998651 .\\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk\\nMichalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model\\ncascades. ArXiv , abs/2207.10342, 2022.\\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and T. Zhang.\\nRaft: Reward ranked finetuning for generative foundation model alignment. ArXiv , abs/2304.06767, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:258170300 .\\nYuqing Du, Alexander Havrilla, Sainbayar Sukhbaatar, Pieter Abbeel, and Roberta Raileanu. A study on improving\\nreasoning in language models. In I Can’t Believe It’s Not Better Workshop: Failure Modes in the Age of Foundation\\nModels , 2023. URL https://openreview.net/forum?id=tCZFmDyPFm .\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,\\nand Tatsunori Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback.\\nArXiv , abs/2305.14387, 2023. URL https://api.semanticscholar.org/CorpusID:258865545 .\\n12Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\\nKevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL\\nhttps://doi.org/10.5281/zenodo.5371628 .\\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International\\nConference on Machine Learning , 2022. URL https://api.semanticscholar.org/CorpusID:252992904 .\\nAmelia Glaese, Nathan McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\\nLaura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang,\\nRamona Comanescu, Fan Yang, A. See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez\\nElias, Richard Green, Sovna Mokr’a, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\\nWilliam S. Isaac, John F. J. Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey\\nIrving. Improving alignment of dialogue agents via targeted human judgements. ArXiv , abs/2209.14375, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:252596089 .\\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya\\nSiddhant, Alexa Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, A. Doucet, Orhan Firat, and Nando\\nde Freitas. Reinforced self-training (rest) for language modeling. ArXiv , abs/2308.08998, 2023. URL https:\\n//api.semanticscholar.org/CorpusID:261031028 .\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\\nSteinhardt. Measuring mathematical problem solving with the math dataset, 2021a.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and\\nJacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv , abs/2103.03874, 2021b.\\nURL https://api.semanticscholar.org/CorpusID:232134851 .\\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora:\\nLow-rank adaptation of large language models. ArXiv , abs/2106.09685, 2021. URL https://api.semanticscholar.\\norg/CorpusID:235458009 .\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language\\nmodels can self-improve. ArXiv , abs/2210.11610, 2022.\\nMinqi Jiang, Edward Grefenstette, and Tim Rockt¨ aschel. Prioritized level replay. In International Conference on\\nMachine Learning , 2020. URL https://api.semanticscholar.org/CorpusID:222208809 .\\nRobert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette,\\nand Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint\\narXiv:2310.06452 , 2023.\\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. Coderl: Mastering code\\ngeneration through pretrained models and deep reinforcement learning. ArXiv , abs/2207.01780, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:250280117 .\\nJoel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley. Evolution through\\nlarge models. 2022. URL https://api.semanticscholar.org/CorpusID:249848020 .\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh,\\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\\nVedant Misra. Solving quantitative reasoning problems with language models. ArXiv , abs/2206.14858, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:250144408 .\\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier\\nDehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo˜ ao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\\nWang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan\\nZhang, Nourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim\\nKunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\\nSchoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan\\nDolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜ noz\\nFerrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\\nsource be with you! ArXiv , abs/2305.06161, 2023. URL https://api.semanticscholar.org/CorpusID:258588247 .\\n13Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian\\nCosgrove, Christopher D. Manning, Christopher R´ e, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin\\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia\\nZheng, Mert Y¨ uksekg¨ on¨ ul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Henderson,\\nQian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard,\\nTianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic\\nevaluation of language models. ArXiv , abs/2211.09110, 2022. URL https://api.semanticscholar.org/CorpusID:\\n263423935 .\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\\nSchulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv , abs/2305.20050, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:258987659 .\\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. Rainier:\\nReinforced knowledge introspector for commonsense question answering. ArXiv , abs/2210.03078, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:252735191 .\\nXiming Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin\\nChoi. Quark: Controllable text generation with reinforced unlearning. ArXiv , abs/2205.13636, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:249152301 .\\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin,\\nShifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models\\nvia reinforced evol-instruct. ArXiv , abs/2308.09583, 2023. URL https://api.semanticscholar.org/CorpusID:\\n261030818 .\\nGr´ egoire Mialon, Cl´ ementine Fourrier, Craig Swift, Thomas Wolf, Yann Andr´ e LeCun, and Thomas Scialom. Gaia: a\\nbenchmark for general ai assistants. 2023. URL https://api.semanticscholar.org/CorpusID:265351664 .\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural\\nlanguage crowdsourcing instructions. In Annual Meeting of the Association for Computational Linguistics , 2021.\\nURL https://api.semanticscholar.org/CorpusID:237421373 .\\nSwaroop Mishra, Pan Lu, and A. Kalyan. Lila: A unified benchmark for mathematical reasoning. 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:257405677 .\\nOpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. URL https://api.semanticscholar.org/CorpusID:\\n257532815 .\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models\\nto follow instructions with human feedback. ArXiv , abs/2203.02155, 2022. URL https://api.semanticscholar.\\norg/CorpusID:246426909 .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems?,\\n2021.\\nXiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark Riedl. Inferring the reader: Guiding automated story generation\\nwith commonsense reasoning, 2021.\\nYujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian,\\nSihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.\\nToolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv , abs/2307.16789, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:260334759 .\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct\\npreference optimization: Your language model is secretly a reward model. ArXiv , abs/2305.18290, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:258959321 .\\nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant´ e Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage,\\nHannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks,\\nbaselines, and building blocks for natural language policy optimization. ArXiv , abs/2210.01241, 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:252693405 .\\n14David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael,\\nand Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. ArXiv , abs/2311.12022, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:265295009 .\\nBaptiste Rozi` ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,\\nTal Remez, J´ er´ emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton\\nFerrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D´ efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis\\nMartin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code,\\n2023.\\nTim Salimans and Richard J. Chen. Learning montezuma’s revenge from a single demonstration. ArXiv , abs/1812.03381,\\n2018. URL https://api.semanticscholar.org/CorpusID:54463584 .\\nTomohiro Sawada, Daniel Paleka, Alex Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay,\\nKshitij Gupta, and Aran Komatsuzaki. Arb: Advanced reasoning benchmark for large language models. ArXiv ,\\nabs/2307.13692, 2023. URL https://api.semanticscholar.org/CorpusID:260155126 .\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess` ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda,\\nand Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv , abs/2302.04761,\\n2023. URL https://api.semanticscholar.org/CorpusID:256697342 .\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\\nalgorithms. ArXiv , abs/1707.06347, 2017. URL https://api.semanticscholar.org/CorpusID:28695052 .\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual\\ndata. ArXiv , abs/1511.06709, 2015. URL https://api.semanticscholar.org/CorpusID:15600925 .\\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\\nZhao, Yuenan Guo, and Qianxiang Wang. Pangu-coder2: Boosting large language models for code with ranking\\nfeedback. ArXiv , abs/2307.14936, 2023. URL https://api.semanticscholar.org/CorpusID:260202985 .\\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,\\nL. Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering\\nchess and shogi by self-play with a general reinforcement learning algorithm. ArXiv , abs/1712.01815, 2017. URL\\nhttps://api.semanticscholar.org/CorpusID:33081038 .\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R.\\nBrown, Adam Santoro, Aditya Gupta, Adri` a Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal,\\nAlethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia\\nParrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Annasaheb Rahane,\\nAnantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew M.\\nDai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta,\\nAnna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun\\nKirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs,\\nB. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia,\\nBehnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Stephen Howald, Bryan\\nOrinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C’esar Ferri Ram’irez, Chandan\\nSingh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian\\nVoigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel,\\nCourtney Ashcraft, Cristina Garbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, Dan Kilman, Dan Roth,\\nDaniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu’i Gonz’alez, Danielle R. Perszyk, Danny Hernandez,\\nDanqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep\\nGanguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra,\\nDilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus\\nCubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth P. Donoway, Ellie Pavlick, Emanuele Rodol` a,\\nEmma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan J. Jerzak, Ethan\\nKim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart’inez-Plumed,\\nFrancesca Happ’e, Fran¸ cois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ´ an\\nKruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L’opez, Gregor Betz,\\nGuy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar,\\nHenry Shevlin, Hinrich Schutze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap\\nJumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern´ andez Fisac, James B. Simon, James\\nKoppel, James Zheng, James Zou, Jan Koco’n, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom,\\nJascha Narain Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer\\n15Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang,\\nJane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg,\\nJos Rozen, Jos´ e Hern´ andez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua Tenenbaum, Joshua S.\\nRule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja\\nMarkert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia\\nShkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan,\\nLianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig\\nSchmidt, Luheng He, Luis Oliveros Col’on, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje\\nter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria\\nJose Ram’irez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt,\\nMatthias Hagen, M’aty’as Schubert, Medina Baitemirova, Melody Arnaud, Melvin Andrew McElrath, Michael A.\\nYee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michal Swkedrowski, Michele\\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Monica Tiwari,\\nMohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, T MukundVarma, Nanyun Peng, Nathan A. Chi,\\nNayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita\\nNangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel,\\nNuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares,\\nParth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter\\nChang, Peter Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, Piyush S. Patil, Pouya Pezeshkpour, Priti Oli,\\nQiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker,\\nRamon Risco, Raphael Milliere, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers,\\nRobert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Lebras, Rosanne Liu, Rowan Jacobs, Rui\\nZhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\\nMohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman,\\nSamuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean\\nCasey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi S. Hamdan, Sharon\\nZhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar,\\nShubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi,\\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan\\nDivic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T Piantadosi, Stuart M. Shieber,\\nSummer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\\nTatsunori Hashimoto, Te-Lin Wu, Theo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius\\nNkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar\\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh\\nRamasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William\\nZhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,\\nYair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\\nYu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zi Fu Wang, Zijie J. Wang, Zirui Wang, and Ziyi\\nWu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:263625818 .\\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario\\nAmodei, and Paul Christiano. Learning to summarize from human feedback. ArXiv , abs/2009.01325, 2020. URL\\nhttps://api.semanticscholar.org/CorpusID:221665105 .\\nSimeng Sun, Dhawal Gupta, and Mohit Iyyer. Exploring the impact of low-rank adaptation on the performance, efficiency,\\nand regularization of rlhf. ArXiv , abs/2309.09055, 2023. URL https://api.semanticscholar.org/CorpusID:\\n261884455 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,\\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\\nfine-tuned chat models, 2023.\\n16Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, L. Wang, Antonia Creswell, Geoffrey\\nIrving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback. ArXiv ,\\nabs/2211.14275, 2022. URL https://api.semanticscholar.org/CorpusID:254017497 .\\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha¨ el Mathieu, Andrew Dudzik, Junyoung Chung, David H.\\nChoi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka,\\nAja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R´ emi Leblond,\\nTobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre,\\nZiyun Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W¨ unsch, Katrina McKinney, Oliver\\nSmith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.\\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature , 575:350 – 354, 2019. URL\\nhttps://api.semanticscholar.org/CorpusID:204972004 .\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\\nand Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv , abs/2109.01652, 2021. URL https:\\n//api.semanticscholar.org/CorpusID:237416585 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou.\\nChain of thought prompting elicits reasoning in large language models. ArXiv , abs/2201.11903, 2022.\\nShunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models\\nfor action generation in text-based games. ArXiv , abs/2010.02903, 2020. URL https://api.semanticscholar.org/\\nCorpusID:222142129 .\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing\\nreasoning and acting in language models. ArXiv , abs/2210.03629, 2022. URL https://api.semanticscholar.org/\\nCorpusID:252762395 .\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models. ArXiv , abs/2305.10601, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:258762525 .\\nAnbang Ye, Christopher Cui, Taiwei Shi, and Mark O. Riedl. Neural story planning. ArXiv , abs/2212.08718, 2022.\\nURL https://api.semanticscholar.org/CorpusID:254854533 .\\nZheng Yuan, Hongyi Yuan, Cheng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on\\nlearning mathematical reasoning with large language models. ArXiv , abs/2308.01825, 2023. URL https://api.\\nsemanticscholar.org/CorpusID:260438790 .\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022.\\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie\\nZhan, and Hongsheng Li. Solving challenging math word problems using gpt-4 code interpreter with code-based\\nself-verification. ArXiv , abs/2308.07921, 2023a. URL https://api.semanticscholar.org/CorpusID:260900008 .\\nWei Zhou, Xiangyu Peng, and Mark O. Riedl. Dialogue shaping: Empowering agents through npc interaction. ArXiv ,\\nabs/2307.15833, 2023b. URL https://api.semanticscholar.org/CorpusID:260333931 .\\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.\\nSolving math word problems via cooperative reasoning induced language models. Association for Computational\\nLinguistics, 2023. doi: 10.18653/v1/2023.acl-long.245. URL https://doi.org/10.18653%2Fv1%2F2023.acl-long.\\n245.\\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\\nGeoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.\\n17Figure 9 Accuracy of EI models on GSM8K test vs.\\nnumber of iterations. EI scratch models have use no SFT\\ninitialization.\\nFigure 10 Accuracy of EI models on GSM8K test vs.\\nnumber of iterations. K= 4 samples per prompt are used\\nto construct a fine-tuning dataset for the next round.\\nA RCRL Label Balance\\nWe also experiment with different proportions of ‘[GOOD]’ and ‘[BAD]’ labels in RCRL training data. This is\\nmotivated by a desire to make better use of abundant negative data, which is much easier to generate than its\\npositive counterpart. Better teaching the student what notto do with this data would ideally increase the\\nnumber of valid solutions. Recall by default we balance the number of positive and negative samples.\\nWe conduct experiments on LLama-2 7 GSM8K without any SFT data. We apply only one round of Expert\\nIteration ( K= 1 per question), producing a student model we refer to as EI-minimal . Note, in this setting we\\nonly provide ‘[GOOD]’ and ‘[BAD]’ labels for entire solutions, rather than providing labels at the step level.\\nResults are reported in 5.\\npositive:negative ratio GSM8K (maj@1)\\nEI-minimal - 0.17\\n100:1 0.18\\nRCRL 10:1 0.18\\n1:1 0.15\\nTable 5 RCRL without SFT, using different proportions of positive and negative samples. As we increase the proportion\\nof negative samples, performance generally decreases. At best, we only see very marginal gains using RCRL . Note:\\nEI-minimal refers to running EI for one iteration, with K= 1 per question.\\nWe find we achieve best performance when the amount of positive training data greatly outweighs the amount\\nof negative data. In these cases, our RCRL models’ maj@1 score slightly exceeds the maj@1 score of the\\ndata generating EI-minimal model. Yet, when we balance the amount of positive and negative training data,\\nwe find performance is degraded. This suggests our 7B student doesn’t effectively learn from the provided\\nnegative demonstrations. We suspect either a larger model or an easier task would give better results.\\nB EI Improvement across Iterations\\nFigures 9 and 10 plot the maj@1 score of models on versus rounds of expert iteration. On both datasets the\\nscore is monotonically increasing until convergence after at most four rounds. Models initialized from an\\nSFT checkpoint converge faster than their pretrained counterparts. Each round of expert iteration samples\\nK∗numtrain rollouts, with the longest running training loop generate at most 5 ∗4∗7000≈106samples.\\n18Figure 11 Diversity of GSM8K model output over rounds of EI. (No SFT)\\nFigure 12 Diversity of SVAMP model output over rounds of EI. K= 96 samples are used per prompt. positive\\ndiversity measures diversity in the subset of solutions with a correct final answer.\\nFigures 11 and 12 report the diversity of solutions across rounds of expert iteration as measured by two\\nseparate metrics for solution uniqueness. exact diversity checks for equality between two solutions using\\nexact string match. trace diversity checks for equality between two solutions by first extracting the trace of a\\nsolution as the sequence of intermediate calculations used to get to the final answer. An exact match is then\\nperformed on this trace representation.\\nSolution diversity increases then decreases over training Overall both measures of solution diversity increase\\nfor both model sizes over the first two rounds of expert iteration. After the first two rounds both trace\\ndiversity appears to plateau and in some cases slightly decrease. Exact diversity continues to increase for\\n13B, but not at the same rate as during the first two rounds. The largest increases in solution diversity over\\nthe first two rounds also match when the largest gains in maj@1 performance occur. This lends evidence to\\nthe intuition that a high-performing student will be able to generate many correct but unique solutionst to\\nthe same problem. Further, we see during later rounds of expert iteration that while maj@1 score improves\\nslightly, diversity suffers. This provides further evidence that training is begining to overfit to maj@1 score, in\\nthe process reducing both pass@n and solution diversity. We see the same behavior\\nLarger models generate more diverse solutions The above figures also demonstrate the 13B model produces\\nsignifcantly more diverse outputs than the 7B model. This is true during every round of fine-tuning, with the\\ngap getting larger as more training is done. Interestingly, the 13B model appears to produce an exactly unique\\nsolution with every sampling after 4 rounds of expert iteration. However, its trace diversity peaks after two\\n19Figure 13 gsm8k sft diversity\\nrounds, indicating 13B tends to introduce semantic diversity without changing the underlying computational\\nstructure of a solution.\\nC Sample Complexities\\nIn this section we plot all sample complexities on benchmarks accompanying the results in Section 4. Figures\\n14 and 15 report results on GSM8K without supervised fine-tuning. Figures 16 and 17 report results on\\nSVAMP.\\nFigure 14 Sample complexity of default versus ORM\\nguided EI students on GSM8K (no SFT). The ORM\\nimproves sample complexity initially but ultimately un-\\nderperforms using only the ground truth.\\nFigure 15 Sample complexity of default versus ORM\\nguided PPO students on GSM8K (no SFT). Similarly to\\nas in EI, the ORM improves maj@1 score over using only\\nground truth rewards but eventually underperforms.\\nAs in the SFT case, using an ORM to guide EI and PPO on prompted GSM8K models does somewhat\\nreduce sample complexity but does not improve best performance (if anything the ORM reward slightly hurts\\nconverged maj@1 score). We see the same story when providing a dense ORM reward, further decreasing\\nsample comlexity but at the cost of final converged performance. Our best results still come from using only\\nthe ground truth score. We suspect the performance degredation introduced by the ORM reward could be\\nalleviated with a larger reward model. However, we do not believe using a larger model would improve over\\njust the ground truth reward. Similar results are seen for SVAMP.\\n20Figure 16 Sample complexity of default versus ORM\\nguided EI students on SVAMP.\\nFigure 17 Sample complexity of default versus ORM\\nguided PPO students on SVAMP.\\nFigure 18 maj@1 scores for Prioritized Level Replay (PLR) and Backtracking techniques compared to default PPO\\nand SFT.\\nD Curriculum Learning for RL\\nIn addition to vanilla PPO we experiment with backtracking (Salimans and Chen, 2018) and Prioritized Level\\nReplay ( PLR) (Jiang et al., 2020) as algorithms from the curriculum learning literature. Such algorithms\\naim to construct a “curriculum” of subproblems, with the model ideally learning to generalize from easier\\nsubproblems to harder subproblems.\\nBacktracking in particular is a natural choice as it relies on using high-quality supervised trajectories to\\nimprove exploration of the solution space. This is done by sampling the student policy πon the partially\\ncomplete solution ( Q, P i) where Piis a sequence of intermediate ground truth steps ( S1, ..., S i). The algorithm\\nproceeds by setting an initial threshold τ0∈(0,1) which represents how far back from the final answer to\\ninitialize partial solutions. By default we use τ0= 0.9. Then, for each problem Qwhich can be solved from\\nPi, we remove the last step Siand condition on Pi−1the next time Qis sampled.\\nPLR does not rely on access to SFT data, instead heuristically prioritizing problems with high “learning\\npotential” estimated by the average absolute advantage. Prioritizing problems with this potential allows the\\nmodel to focus on problems that are neither too easy nor too hard, making efficient use of its exploration\\nbudget. We initialize the student using a supervised fine-tuned LLama-2 7B on GSM8K. Results are reported\\nin Figure 19.\\n21Figure 19 maj@1 scores on GSM8K for Prioritized Level Replay (PLR) and Backtracking techniques compared to\\ndefault PPO and SFT.\\nOverall we find neither method exceeds the performance of default PPO. We hypothesize this is due to the\\nlimited exploration the model engages in from the start, due to both pretraining and supervised fine-tuning.\\nWe speculate better results might be achieved on a harder dataset with more intermediate steps, particularly\\nwhen using backtracking.\\nE Data augmentation\\nWe additionally experimented with generating synthetic ( Q, A) training pairs via an approach inspired by\\nbacktranslation (Sennrich et al., 2015). We assume access to a supervised fine-tuning dataset Dof (Q, A)\\npairs and train a Q→Amodel MQ→Aas our usual student model. We call this model the verifier. We can\\nalso utilize Dto train models of the form MA→QandMA→Awhich map answers to questions and answers\\nto answers respectively. We train MA→Qsimply by fine-tuning the pretrained model Mto predict p(A|Q)\\nwhere ( Q, A)∼ D. We call the combination of MA→AandMA→Qthe generator. We construct a train set\\nforMA→Aas follows: For each Ain (Q, A)∈ Dwe randomly sample three other answers A1, A2, A3fromD\\nwhich act as a conditional prompt. We then train MA→Aby minimizing p(A|A1, A2, A3).\\nWe sample MA→Aon each ground truth answer A∈ DK= 8 times, constructing a synthetic dataset of\\nanswers A. We then use our backwards model MA→Qto produce questions for each of the synthetic answers\\nA∈ A. This forms a synthetic ( Q, A) dataset Dsynth. Finally, for each synthetic ( Q, A) pair, we sample our\\nstudent model MQ→AK= 20 times for each question and check whether the student model’s final answer\\nagrees with the “intended” final answer. We refer to the percentage of student generated solutions recovering\\nthe intended final answer as the score for a synthetic ( Q, A) pair. We plot the distribution of scores in Figure\\n20.\\nWe see that the majority of synthetic pairs, over 50,000, never have their solutions recovered by the student\\nMQ→A. This is either because a) the student is too weak to solve the question or b) the question is impossible\\nto solve. Either way, we likely do not want to include these new training data for the student. Similarly, we\\nlikely do not want to include questions which are always solved by the student, i.e. those with score = 1, as\\nthey are too easy. Additionally, we should be wary of questions which have a small score in the range (0 , ϵ).\\nWe expect many questions will have been solved incorrectly but still arriving at the correct final answer. We\\nshould exclude such problems from our training dataset.\\nWe expect the highest quality data ( Q, A) to have a score in the neighborhood (1\\n2−τ,1\\n2+τ). These questions\\nshould be not too hard but not too easy for our student. Figure 6 shows the performance of student models\\nfine-tuned on a combination of ground truth data and synthetically generated data with scores in the range\\n(1\\n2−τ,1\\n2+τ). All models are trained for five epochs with an initial lr = 2e-5 cosine decayed to 2e-7. Llama-2\\n7B is used as the pretrained base model.\\n22Figure 20 Scores of synthetically backwards generated ( Q, A) pairs. Note: the score refers to the percentage of times\\nthe forward student model MQ→Arecovers the intended final answer.\\nmaj@1\\nτ= 0.1 0.38\\nτ= 0.2 0.36\\nτ= 0.3 0.34\\nSFT 0.41\\nTable 6 Performance of models training with various amounts of synthetic data vs. the SFT baseline. Note: τ\\nrepresents the size of the neighborhood of scores around1\\n2that are not filtered out.\\nUnfortunately, it seems introducing any amount of synthetically generated data degrades performance. When\\nmanually inspecting the synthetically generated ( Q, A) pairs it becomes clear why. There is an extremely\\nhigh number of false positives. Consider the following example of a synthetic pair shown in Table ??:\\nThis is an example of a low-quality sample we do not want in our training data. Ideally, such a sample would\\nhave a score of 0 since the technically correct answer is 100, not 120. However, the SFT MQ→Astudent we\\nuse to construct a score for each ( Q, A) sample computes the final answer as 120 a staggering 47% of the time.\\nThe verifier makes the exactly the same mistakes the MA→Amodel made when constructing the question,\\nlikely because they were trained on similar distributions.\\nWe suspect using a larger model more capable of detecting these sort of trivial non-solutions would do\\nsubstantially better at generating backwards synthetic data. Similarly, employing separate models as the\\ngenerator and verifier may reduce the probability of both making the same mistakes, improving the reliability\\nof the score for each pair. We leave this as future work.\\nF RCRL Step-label Generating Process\\nAnother natural candidate which could be used to identify mistakes at each step is a Process Based Reward\\nModel (PRM) (Lightman et al., 2023). A PRM estimates the probability of correctness of a step Si,\\np(Sicorrect |Q, S 1, S2, ..., S i) independently of its impact on the final answer. However, this would be\\nexpensive, requiring collecting human annotated samples. Instead, we propose to approximate the optimal\\nvalue function V∗of the reasoning task. V∗corresponds to the value function of the optimal policy which is\\nable to successfully solve the reasoning task from any logically valid intermediate state Sj. Such an optimal\\nvalue function would have V∗(Q, S 1, ..., S i) = 1 for a solution prefix with no mistakes, and V∗(Q, S 1, ..., S i) = 0\\nif the prefix already contains a mistake which will result in an incorrect final answer. Note however, V∗does\\n23Question ”A school of 100 musicians goes on a skiing trip. 40%\\nare beginners, 30% are intermediate, and 50% are\\nadvanced. How many people went on the skiing trip?”\\nAnswer ”There are 100 * 0.4 = 40 beginner skiiers. There are\\n100 * 0.3 = 30 intermediate skiiers. There are 100 *\\n0.5 = 50 advanced skiiers. Therefore there are 40 +\\n30 + 50 = 120 skiiers total.”\\nnot exactly correspond to a PRM. This is because a partial solution S1, ..., S iwith a mistake at step j̸=iand\\nvalid terminal step Siwill have V∗(Q, S 1, ..., S i) = 0 and PRM (Q, S 1, ..., S i) = 1. To make this distinction\\nclear, we call models we train to directly approximate V∗stepwise ORMs or SORMs .\\n24'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1c673e12110>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Guild\\TDL_TA\\.conda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This article is a collection of research papers and articles on various topics in the field of natural language processing and artificial intelligence, written by a team of authors from different institutions and backgrounds. The article includes papers on topics such as question answering, text generation, and numerical reasoning. Some of the authors mentioned in the article are Aarohi Srivastava, Abhinav Rastogi, Jiacheng Liu, and Francois Chollet. '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "query = \"Give me the introduction for this article\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
